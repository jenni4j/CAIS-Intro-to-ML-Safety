{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lOgLA7wnveQy"
      },
      "source": [
        "# Adversarial Robustness v0.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yIHHqFDAU1Uy"
      },
      "source": [
        "## Introduction & Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdYeVWEv4pm"
      },
      "source": [
        "Adversarial examples are examples designed in order to cause an machine\n",
        "\n",
        "---\n",
        "\n",
        "learning system to malfunction. Here, an adversary is taking a real image of a panda and adds some adversarially generated noise to get the adversarial example. The adversarial noise is designed to have small distance from the original image, so it still looks like a panda for humans. However, the model now believes its a gibbon with 99.3\\% confidence.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1kvJRRUDssx8ZarAH71-nxv2c2_RBNz4G)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULnrQQMziFWW"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CsonckZATb0A"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7VaQEXDqLT6"
      },
      "outputs": [],
      "source": [
        "# Cloning the files from github\n",
        "\n",
        "!git clone --branch adversarial https://github.com/oliverzhang42/safety.git\n",
        "!pip3 install git+https://github.com/MadryLab/robustness.git\n",
        "!pip3 install torchvision=='0.10.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fpp4c-Qu_L5"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from safety.utils import utils\n",
        "from safety.lesson1 import adversarial\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O8GlEfyiUkiQ"
      },
      "source": [
        "## First Adversarial Attack using FGSM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra_XR_pQUqwY"
      },
      "source": [
        "### Untargeted FGSM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CeilpN4uwAcV"
      },
      "source": [
        "The first method we look at is the untargeted Fast Gradient Sign Method (FGSM) proposed by [Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf). The attack constructs adversarial examples as follows:\n",
        "\n",
        "$$x_\\text{adv} = x + \\epsilon\\cdot\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
        "\n",
        "where\n",
        "\n",
        "*   $x_\\text{adv}$ : Adversarial image.\n",
        "*   $x$ : Original input image.\n",
        "*   $y$ : Original input label.\n",
        "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
        "*   $\\theta$ : Model parameters.\n",
        "*   $J$ : Loss.\n",
        "\n",
        "The current attack formulation is considered 'untargeted' because it only seeks to maximize loss rather than to trick the model into predicting a specific label.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K97qFMRUxloZ"
      },
      "source": [
        "Try implementing the untargeted FGSM method for a batch of images yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfc8ZJvov_25"
      },
      "outputs": [],
      "source": [
        "def untargeted_FGSM(x_batch, true_labels, network, normalize, eps=8/255., **kwargs):\n",
        "  \"\"\"Generates a batch of untargeted FGSM adversarial examples\n",
        "\n",
        "  Args:\n",
        "    x_batch (torch.Tensor): the batch of unnormalized input examples.\n",
        "    true_labels (torch.Tensor): the batch of true labels of the example.\n",
        "    network (nn.Module): the network to attack.\n",
        "    normalize (function): a function which normalizes a batch of images\n",
        "        according to standard imagenet normalization.\n",
        "    eps (float): the bound on the perturbations.\n",
        "  \"\"\"\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(reduce=\"mean\")\n",
        "  x_batch.requires_grad = True\n",
        "  output = network(x_batch)\n",
        "  loss = loss_fn(output, true_labels)\n",
        "  network.zero_grad()\n",
        "  loss.backward()\n",
        "  sign_grad = x_batch.grad.sign()\n",
        "  x_adv = x_batch + eps * sign_grad\n",
        "\n",
        "  return x_adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QhSVlcQy3N2"
      },
      "outputs": [],
      "source": [
        "# Test the method\n",
        "adversarial.test_untargeted_attack(untargeted_FGSM, eps=8/255.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz5jFmrokrg2"
      },
      "source": [
        "If things go well, the model should switch from predicting 'giant panda' to predicting 'brown bear' or some other class. Additionally, try increasing the epsilon to see the noise more clearly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3r_F67UwZl"
      },
      "source": [
        "### Targeted FGSM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BcMdHeO_g8c3"
      },
      "source": [
        "In addition to the untargeted FGSM which simply seeks to maximize loss, we can also create targeted adversarial attacks. We do this using the following equation:\n",
        "\n",
        "$$x_{adv} = x - \\epsilon\\cdot\\text{sign}(\\nabla_xJ(\\theta, x, y_{target}))$$\n",
        "\n",
        "where\n",
        "\n",
        "*   $x_{adv}$ : Adversarial image.\n",
        "*   $x$ : Original input image.\n",
        "*   $y_{target}$ : The target label.\n",
        "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
        "*   $\\theta$ : Model parameters.\n",
        "*   $J$ : Loss."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R6qsvp2qRvpY"
      },
      "source": [
        "Try implementing the targeted FGSM method for a batch of images yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtzSvRS6g74j"
      },
      "outputs": [],
      "source": [
        "def targeted_FGSM(x_batch, target_labels, network, normalize, eps=8/255., **kwargs):\n",
        "  \"\"\"Generates a batch of targeted FGSM adversarial examples\n",
        "\n",
        "  Args:\n",
        "    x_batch (torch.Tensor): the unnormalized input example.\n",
        "    target_labels (torch.Tensor): the labels the model will predict after the attack.\n",
        "    network (nn.Module): the network to attack.\n",
        "    normalize (function): a function which normalizes a batch of images\n",
        "        according to standard imagenet normalization.\n",
        "    eps (float): the bound on the perturbations.\n",
        "  \"\"\"\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(reduce=\"mean\")\n",
        "  x_batch.requires_grad = True\n",
        "  output = network(x_batch)\n",
        "  loss = loss_fn(output, target_labels)\n",
        "  network.zero_grad()\n",
        "  loss.backward()\n",
        "  sign_grad = x_batch.grad.sign()\n",
        "  x_adv = x_batch + eps * sign_grad\n",
        "\n",
        "  return x_adv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "diwyVHJ2Z8dA"
      },
      "source": [
        "**Note that even if the implementation is perfect, FGSM is not able to generate effective targeted attacks, so don't expect the output image to assign a high probability to the target label.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TWSw_jqhBkY"
      },
      "outputs": [],
      "source": [
        "adversarial.test_targeted_attack(targeted_FGSM, target_idx=8, eps=8/255.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MjIkbCwUwDcm"
      },
      "source": [
        "## Additional Adversarial Attacks\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UGWMppidPKd0"
      },
      "source": [
        "## Implementing L2 normalization and clamping"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MFhVbVriQpZl"
      },
      "source": [
        "We will implement some helper functions that we can use for the Projected Gradient Descent (PGD) L2 method below.\n",
        "\n",
        "For the normalize_l2 function we will be returning the following value\n",
        "\n",
        "$$\\frac{x}{||x||_{2}}$$\n",
        "\n",
        "For the tensor_clamp_l2 function we will compute and return the following value\n",
        "\n",
        "\\begin{equation}\n",
        "    X=\n",
        "    \\begin{cases}\n",
        "      clamp(x), & \\text{if}\\ ||x-c||_2 > r \\\\\n",
        "      x, & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "  \\end{equation}\n",
        "\n",
        "where\n",
        "$ \\text{clamp}(x) = c + \\frac{x-c}{||x-c||_2} ⋅ r$, X is the return value, x is the input, c (center) is a tensor of the same shape as x, and r (radius) is a scalar value."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E5oK5-POSAlY"
      },
      "source": [
        "Try implementing the batched version of normalize_l2 and tensor_clamp_l2 below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In0l1Fl8QJLB"
      },
      "outputs": [],
      "source": [
        "def normalize_l2(x_batch):\n",
        "    \"\"\"\n",
        "    Expects x_batch.shape == [N, C, H, W]\n",
        "    where N is the batch size,\n",
        "    C is the channels (or colors in our case),\n",
        "    H, W are height and width respectively.\n",
        "\n",
        "    Note: To take the l2 norm of an image, you will want to flatten its dimensions (be careful to preserve the batch dimension of x_batch).\n",
        "    \"\"\"\n",
        "    batch_norm = torch.norm(x_batch.view(x_batch.shape[0], -1), dim=1, keepdim=True)\n",
        "    x_normalized = x_batch / batch_norm.view(-1, 1, 1, 1)\n",
        "\n",
        "    return x_normalized\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHU9UKZjQUC1"
      },
      "outputs": [],
      "source": [
        "def tensor_clamp_l2(x_batch, center, radius):\n",
        "    \"\"\"Batched clamp of x into l2 ball around center of given radius.\"\"\"\n",
        "\n",
        "    delta = x_batch - center\n",
        "\n",
        "    batch_norm = torch.norm(delta.view(delta.shape[0], -1), dim=1, keepdim=True)\n",
        "\n",
        "    if batch_norm > radius:\n",
        "      x_clamped = center + (delta / batch_norm) * radius\n",
        "    else:\n",
        "      x_clamped = x_batch\n",
        "\n",
        "    return x_clamped\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8memMEKaQwkS"
      },
      "outputs": [],
      "source": [
        "def PGD_l2(x_batch, true_labels, network, normalize, num_steps=20, step_size=3./255, eps=128/255., **kwargs):\n",
        "        \"\"\"\n",
        "        :return: perturbed batch of images\n",
        "        \"\"\"\n",
        "        # Initialize our adversial image\n",
        "        x_adv = x_batch.detach().clone()\n",
        "        x_adv += torch.zeros_like(x_adv).uniform_(-eps, eps)\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "            x_adv.requires_grad_()\n",
        "\n",
        "            # Calculate gradients\n",
        "            with torch.enable_grad():\n",
        "              logits = network(normalize(x_adv))\n",
        "              loss = F.cross_entropy(logits, true_labels, reduction='sum')\n",
        "\n",
        "            # Normalize the gradients with your L2\n",
        "            grad = normalize_l2(torch.autograd.grad(loss, x_adv, only_inputs=True)[0])\n",
        "\n",
        "            # Take a step in the gradient direction.\n",
        "            x_adv = x_adv.detach() + step_size * grad\n",
        "            # Project (by clamping) the adversarial image back onto the hypersphere\n",
        "            # around the image.\n",
        "            x_adv = tensor_clamp_l2(x_adv, x_batch, eps).clamp(0, 1)\n",
        "\n",
        "        return x_adv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ooO0rDqzQodI"
      },
      "source": [
        "Try out the helper functions you wrote. Note how the hyperparameters differ depending on the attack that one is using. You can see more examples below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5CVUqJ4gywi"
      },
      "outputs": [],
      "source": [
        "adversarial.test_untargeted_attack(PGD_l2, eps=128/255.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PqZBCsVTU-cm"
      },
      "source": [
        "In addition to your implementations of FGSM, we will provide you with an\n",
        "\n",
        "---\n",
        "\n",
        "implementation of Projected Gradient Descent (PGD) by [Madry et al.](https://arxiv.org/pdf/1706.06083.pdf). As mentioned in the lecture, PGD can be seen a stronger version of FGSM which applies FGSM many times. We provide both targeted and untargeted versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySu3xkNe2S4w"
      },
      "outputs": [],
      "source": [
        "def untargeted_PGD(x_batch, true_labels, network, normalize, num_steps=10, step_size=0.01, eps=8/255., **kwargs):\n",
        "  \"\"\"Generates a batch of untargeted PGD adversarial examples\n",
        "\n",
        "  Args:\n",
        "    x_batch (torch.Tensor): the batch of unnormalized input examples.\n",
        "    true_labels (torch.Tensor): the batch of true labels of the example.\n",
        "    network (nn.Module): the network to attack.\n",
        "    normalize (function): a function which normalizes a batch of images\n",
        "        according to standard imagenet normalization.\n",
        "    num_steps (int): the number of steps to run PGD.\n",
        "    step_size (float): the size of each PGD step.\n",
        "    eps (float): the bound on the perturbations.\n",
        "  \"\"\"\n",
        "  x_adv = x_batch.detach().clone()\n",
        "  x_adv += torch.zeros_like(x_adv).uniform_(-eps, eps)\n",
        "\n",
        "  for i in range(num_steps):\n",
        "    x_adv.requires_grad_()\n",
        "\n",
        "    # Calculate gradients\n",
        "    with torch.enable_grad():\n",
        "      logits = network(normalize(x_adv))\n",
        "      loss = F.cross_entropy(logits, true_labels, reduction='sum')\n",
        "    grad = torch.autograd.grad(loss, x_adv, only_inputs=True)[0]\n",
        "\n",
        "    # Perform one gradient step\n",
        "    x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "\n",
        "    # Project the image to the ball.\n",
        "    x_adv = torch.maximum(x_adv, x_batch - eps)\n",
        "    x_adv = torch.minimum(x_adv, x_batch + eps)\n",
        "\n",
        "  return x_adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsFo74my5McU"
      },
      "outputs": [],
      "source": [
        "adversarial.test_untargeted_attack(untargeted_PGD, eps=8/255.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI8veDcI8q4L"
      },
      "outputs": [],
      "source": [
        "def targeted_PGD(x_batch, target_labels, network, normalize, num_steps=100, step_size=0.01, eps=8/255., **kwargs):\n",
        "  \"\"\"Generates a batch of untargeted PGD adversarial examples\n",
        "\n",
        "  Args:\n",
        "    x_batch (torch.Tensor): the batch of preprocessed input examples.\n",
        "    target_labels (torch.Tensor): the labels the model will predict after the attack.\n",
        "    network (nn.Module): the network to attack.\n",
        "    normalize (function): a function which normalizes a batch of images\n",
        "        according to standard imagenet normalization.\n",
        "    num_steps (int): the number of steps to run PGD.\n",
        "    step_size (float): the size of each PGD step.\n",
        "    eps (float): the bound on the perturbations.\n",
        "  \"\"\"\n",
        "  x_adv = x_batch.detach().clone()\n",
        "  x_adv += torch.zeros_like(x_adv).uniform_(-eps, eps)\n",
        "\n",
        "  for i in range(num_steps):\n",
        "    x_adv.requires_grad_()\n",
        "\n",
        "    # Calculate gradients\n",
        "    with torch.enable_grad():\n",
        "      logits = network(normalize(x_adv))\n",
        "      loss = F.cross_entropy(logits, target_labels, reduction='sum')\n",
        "    grad = torch.autograd.grad(loss, x_adv, only_inputs=True)[0]\n",
        "\n",
        "    # Perform one gradient step\n",
        "    # Note that this time we use gradient descent instead of gradient ascent\n",
        "    x_adv = x_adv.detach() - step_size * torch.sign(grad.detach())\n",
        "\n",
        "    # Project the image to the ball\n",
        "    x_adv = torch.maximum(x_adv, x_batch - eps)\n",
        "    x_adv = torch.minimum(x_adv, x_batch + eps)\n",
        "\n",
        "  return x_adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxst_65_-Hlo"
      },
      "outputs": [],
      "source": [
        "# Try changing the target_idx around!\n",
        "adversarial.test_targeted_attack(targeted_PGD, target_idx=1, eps=8/255.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QNBKgJFoT46C"
      },
      "source": [
        "## Attacks on Adversarially Trained Models\n",
        "We devote this section to attacking an adversarially trained model. As a reminder, a model which has been \"adversarially trained\" means that it has been exposed to a load of adversarial examples over training and has specifically trained to recognize them properly.\n",
        "\n",
        "In this section, we hope to demonstrate that adversarial attacks look a lot different if you're attacking an adversarially trained model.\n",
        "\n",
        "The model we use was taken from this repository and is an L∞ robust ResNet18 trained with adversarial examples of ϵ=8/255."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXJnGnYT-kO"
      },
      "source": [
        "### Attacking Normally Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y90rcLnOUBGQ"
      },
      "outputs": [],
      "source": [
        "# Attack a normal model (we only support targeted methods)\n",
        "adversarial.attack_normal_model(\n",
        "    targeted_PGD,\n",
        "    target_idx=10,\n",
        "    eps=8/255.,\n",
        "    num_steps=10,\n",
        "    step_size=0.01\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XMX3gyZ_UEgw"
      },
      "source": [
        "### Attacking Adversarially Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCPhy_s2UEWI"
      },
      "outputs": [],
      "source": [
        "# Attack an adversarially trained model (we only support targeted methods)\n",
        "adversarial.attack_adversarially_trained_model(\n",
        "    targeted_PGD,\n",
        "    target_idx=10,\n",
        "    eps=8/255.,\n",
        "    num_steps=10,\n",
        "    step_size=0.01\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1ORZQSCpUMWA"
      },
      "source": [
        "### Comparing Adversarial Attacks against different models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rukB5d4wUM-b"
      },
      "source": [
        "Take a few minutes to play around with the previous code. Jot down three observations about how attacking an adversarially trained model differs from attacking a normal model.\n",
        "\n",
        "Example responses:\n",
        "\n",
        "1. The confidence of typical models is higher than adversarially trained models\n",
        "2. [Fill in Observation]\n",
        "3. [Fill in Observation]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XEhu6l94UiwP"
      },
      "source": [
        "## Accuracy vs Number of PGD Steps\n",
        "\n",
        "In this section, we seek to see how accuracy varies as we change the number of steps in PGD. Your first task is to write a function which calculates the model's accuracy on adversarially generated images. For this case, we use untargeted PGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI0DVNFHUuym"
      },
      "outputs": [],
      "source": [
        "def adversarial_accuracy(x_batch, true_labels, network, normalize, num_steps=10, step_size=0.01, eps=8/255.):\n",
        "    \"\"\"Generates a batch of adversarial examples using `untargeted_PGD`. Then\n",
        "    calculates and returns accuracy on said batch of examples.\n",
        "\n",
        "    Args:\n",
        "      x_batch (torch.Tensor): the batch of preprocessed input examples.\n",
        "      true_labels (torch.Tensor): the batch of true labels of the example.\n",
        "      network (nn.Module): the network to attack.\n",
        "      normalize (function): a function which normalizes a batch of images\n",
        "          according to standard imagenet normalization.\n",
        "      num_steps (int): the number of steps to run PGD.\n",
        "      step_size (float): the size of each PGD step.\n",
        "      eps (float): the bound on the perturbations.\n",
        "\n",
        "    Note: Consider the networks prediction to be the class with the highest output (aka logit).\n",
        "    \"\"\"\n",
        "    x_adv = untargeted_PGD(x_batch, true_labels, network, normalize, num_steps=num_steps, step_size=step_size, eps=eps)\n",
        "\n",
        "    #########################\n",
        "    # Enter Your Code Here! #\n",
        "    #########################\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OKP8PyalVBeU"
      },
      "source": [
        "Then, use the previous function to plot the accuracy against the number of PGD steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcS9XNFbVCXr"
      },
      "outputs": [],
      "source": [
        "x_batch = torch.load('safety/lesson1/imagenet_batch').cuda()\n",
        "true_labels = torch.load('safety/lesson1/imagenet_labels').cuda()\n",
        "network = adversarial.get_adv_trained_model().eval()\n",
        "normalization_function = utils.IMAGENET_NORMALIZE\n",
        "\n",
        "#########################\n",
        "# Enter Your Code Here! #\n",
        "#########################\n",
        "\n",
        "# call adversarial_accuracy(...) for several num_steps and plot"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
